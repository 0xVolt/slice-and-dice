# -*- coding: utf-8 -*-
"""fine-tune-t5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wab5biqIWkuEcGHY-a20xzuAltlS4uo-

# Pre-runtime Checks

## Checking what Type of GPU is Allocated to the Runtime
"""

!nvidia-smi

"""It's a good thing to note that if you're not happy with the GPU allocated to your runtime, you can reset the Colab runtime to try your luck with a better GPU.

# Import Libraries

## Install Required Libraries
"""

!pip install --quiet transformers
!pip install --quiet pytorch-lightning
!pip install --quiet rouge_score

"""## Import the Rest"""

import json
import pandas as pd
import numpy as np

import seaborn as sns

from pylab import rcParams

import matplotlib.pyplot as plt
from matplotlib import rc

from pathlib import Path

import torch
from torch.utils.data import Dataset, DataLoader

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

from sklearn.model_selection import train_test_split

from termcolor import colored

import textwrap

from transformers import (
    AdamW,
    T5ForConditionalGeneration,
    T5TokenizerFast as T5Tokenizer
)

from tqdm.auto import tqdm 

from rouge_score import rouge_scorer
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from sklearn.metrics import f1_score

"""## Configure Plotting Libraries"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

rcParams['figure.figsize'] = 16, 10

"""## Set a Global Seed Value

This ensures that everything in this notebook is reproducable.
"""

pl.seed_everything(42)

"""# Import the Dataset
Using [this Kaggle dataset](https://www.kaggle.com/datasets/sunnysai12345/news-summary) on _News Summaries_ to train our T5 model.
"""

!gdown --id 1NvrlwoYfPSj92igbh-7XIELRsXxBdVKU

df = pd.read_csv('news_summary.csv', encoding='latin-1')
df.head()

"""## Get the Data to our Specifications"""

# Get only the columns that are needed.
df = df[['text', 'ctext']]

# Replace the column names.
df.columns = ['summary', 'text']

# Drop rows that don't have a value.
df = df.dropna()

df.head()

df.shape

"""# Apply Train-test Split

Here we split the dataset into training and testing dataframes.
"""

train_df, test_df = train_test_split(df, test_size=0.1)

print(f"Shape of the Train Set: {train_df.shape}\nShape of the Test Set: {test_df.shape}")

"""# Create a Pytorch Dataset using a Class"""

class NewsSummaryDataset(Dataset):
    def __init__(self, 
        data: pd.DataFrame, 
        tokenizer: T5Tokenizer, 
        text_max_token_len: int=512, 
        summary_max_token_len: int=128
    ):
        """
        A dataset that represents news articles and their respective summaries.

        Args:
        - data (pd.DataFrame): The data that contains the news articles and their summaries.
        - tokenizer (transformers.tokenization_*) : The tokenizer used to tokenize the text and summary.
        - text_max_token_len (int, optional): The maximum length of the text in terms of tokens. Defaults to 512.
        - summary_max_token_len (int, optional): The maximum length of the summary in terms of tokens. Defaults to 128.
        """
        self.tokenizer = tokenizer
        self.data = data
        self.text_max_token_len = text_max_token_len
        self.summary_max_token_len = summary_max_token_len
        
    def __len__(self):
        """
        Returns:
        - The number of samples in the dataset.
        """
        return len(self.data)
    
    def __getitem__(self, index):
        """
        Get a sample from the dataset.

        Args:
        - index (int): The index of the sample to get.

        Returns:
        - A dictionary that contains the following:
            - text (str): The original text of the news article.
            - summary (str): The summary of the news article.
            - text_input_ids (torch.Tensor): The input IDs of the text after tokenization.
            - text_attention_mask (torch.Tensor): The attention mask of the text after tokenization.
            - labels (torch.Tensor): The input IDs of the summary after tokenization.
            - labels_attention_mask (torch.Tensor): The attention mask of the summary after tokenization.
        """
        data_row = self.data.iloc[index]
        text = data_row["text"]

        # Encode the text.
        text_encoding = self.tokenizer(
            text, 
            max_length=self.text_max_token_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )

        # Encode the summary.
        summary_encoding = self.tokenizer(
            data_row["summary"], 
            max_length=self.summary_max_token_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )

        # Modify the labels so that the model knows which tokens to predict.
        labels = summary_encoding['input_ids']
        labels[labels == 0] = -100
        
        # Return a dictionary.
        return {
            'text': text,
            'summary': data_row['summary'],
            'text_input_ids': text_encoding['input_ids'].flatten(),
            'text_attention_mask': text_encoding['attention_mask'].flatten(),
            'labels': labels.flatten(),
            'labels_attention_mask': summary_encoding["attention_mask"].flatten()
        }

"""# Create Data Module for Pytorch Lightning"""

class NewsSummaryDataModule(pl.LightningDataModule):
    def __init__(
        self,
        train_df,
        test_df,
        tokenizer,
        batch_size=8,
        text_max_token_len=512,
        summary_max_token_len=128
    ):
        """
        Initializes the NewsSummaryDataModule.
        
        Args:
        - train_df (pandas.DataFrame): the training dataset
        - test_df (pandas.DataFrame): the testing dataset
        - tokenizer (transformers.PreTrainedTokenizer): the tokenizer to be used
        - batch_size (int): the batch size
        - text_max_token_len (int): the maximum number of tokens for the text
        - summary_max_token_len (int): the maximum number of tokens for the summary
        """
        super().__init__()
        
        self.train_df = train_df
        self.test_df = test_df
        
        self.batch_size = batch_size
        self.tokenizer = tokenizer
        self.text_max_token_len = text_max_token_len
        self.summary_max_token_len = summary_max_token_len
    
    def setup(self, stage=None):
        """
        Sets up the dataset.
        """
        self.train_dataset = NewsSummaryDataset(
            self.train_df,
            self.tokenizer,
            self.text_max_token_len,
            self.summary_max_token_len)
        
        self.test_dataset = NewsSummaryDataset(
            self.test_df,
            self.tokenizer,
            self.text_max_token_len,
            self.summary_max_token_len)
    
    def train_dataloader(self):
        """
        Returns the DataLoader for the training set.
        """
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=2
        )
    
    def test_dataloader(self):
        """
        Returns the DataLoader for the testing set.
        """
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=2
        )
    
    def val_dataloader(self):
        """
        Returns the DataLoader for the validation set, which is the same as the testing set.
        """
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=2
        )

"""# Load the Pre-trained Model"""

MODEL_NAME = "t5-base"

tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

"""# Working with Tokens

## Get the Token Counts for the Text and Summary

This is just to check the distribution of the tokens. It is understandable that the length of these is larger than what the model is allowed to take.
"""

text_token_counts = [len(tokenizer.encode(row["text"])) for _, row in train_df.iterrows()]

summary_token_counts = [len(tokenizer.encode(row["summary"])) for _, row in train_df.iterrows()]

"""## Plot the Distribution of the Tokens"""

fig, (ax1, ax2) = plt.subplots(1, 2)

sns.histplot(text_token_counts, ax=ax1)
ax1.set_title("Distribution of Text Token Counts")
ax1.set_xlabel("Number of Tokens")
ax1.set_ylabel("Frequency")
# ax1.grid(axis='y', alpha=0.5)

sns.histplot(summary_token_counts, ax=ax2)
ax2.set_title("Distribution of Summary Token Counts")
ax2.set_xlabel("Number of Tokens")
ax2.set_ylabel("Frequency")
ax2.grid(axis='y', alpha=0.5)

plt.suptitle("Token Count Distributions")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""From what we can tell from these distributions, the largest number of tokens lie in the 1200 - 1500 range. As for the summaries, the distribution fits close to 100% of samples within 140 tokens meaning that the summaries themselves are small, consise and are without any loss of information.

# Define the Model

## Define the Hyperparameters
"""

N_EPOCHS = 2
BATCH_SIZE = 8

data_module = NewsSummaryDataModule(
    train_df, 
    test_df,
    tokenizer,
    batch_size=BATCH_SIZE
)

"""## Construct the Model"""

class NewsSummaryModel(pl.LightningModule):
    def __init__(self):
        super().__init__()

        # Create an instance of the pre-trained T5 model
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

    # Overwrite the forward() method
    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):
        # Apply the model to the input IDs
        output = self.model(
            input_ids,
            attention_mask=attention_mask,
            labels=labels,
            decoder_attention_mask=decoder_attention_mask
        )

        return output.loss, output.logits

    def shared_step(self, batch, batch_idx, stage):
        input_ids = batch['text_input_ids']
        attention_mask = batch["text_attention_mask"]
        labels = batch["labels"]
        labels_attention_mask = batch["labels_attention_mask"]

        loss, _ = self(
            input_ids=input_ids,
            attention_mask=attention_mask,
            decoder_attention_mask=labels_attention_mask,
            labels=labels
        )

        self.log(f"{stage}_loss", loss, prog_bar=True, logger=True)
        return loss

    # Step methods required by Pytorch Lightning
    def training_step(self, batch, batch_idx):
        return self.shared_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self.shared_step(batch, batch_idx, 'val')

    def test_step(self, batch, batch_idx):
        return self.shared_step(batch, batch_idx, 'test')

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr=0.0001)

"""## Create an Instance of our Model"""

model = NewsSummaryModel()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./lighting_logs/news-summary

callbacks = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode='min'
)

logger = TensorBoardLogger("lightning_logs", name="news-summary")

trainer = pl.Trainer(
    logger=logger,
    callbacks=callbacks,
    max_epochs=N_EPOCHS
    # gpus=1,
    # progress_bar_refresh_rate=30
)

"""# Fit the Data to the Model and Start Training"""

trainer.fit(model, data_module)

"""# Freeze the Best Performing Model from the Last Epoch"""

trained_model = NewsSummaryModel.load_from_checkpoint(
    trainer.checkpoint_callback.best_model_path
)

trained_model.freeze()

"""# Set the Model and the Data to be on the Same Device

We move the model to the CPU after it's been trained on the GPU.
"""

# Set the device to a GPU if available, otherwise use the CPU
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = 'cpu'

# Move the model to the device
trained_model.to(device)

# Move the data to the device
# tensorTestDf = torch.tensor(test_df.values)
# tensorTestDf = tensorTestDf.to(device)

"""# Save the Model's Weights

## Locally in the Runtime
"""

pathToWeights = 'fine-tuned-t5-weights.pt'
torch.save(trained_model.state_dict(), pathToWeights)

"""## Saving it to Google Drive"""

from google.colab import drive
drive.mount('/content/gdrive')

# Save model weights to Google Drive
PATH = '/content/gdrive/MyDrive/Models/fine-tuned-t5-weights.pt'
torch.save(model.state_dict(), PATH)

"""# Performing Summarization

## Define Function to Summarize
"""

def summarize(text):
    text_encoding = tokenizer(
        text,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    # Probably perform hyperparameter optimisation here.
    generated_ids = trained_model.model.generate(
        input_ids=text_encoding['input_ids'],
        attention_mask=text_encoding['attention_mask'],
        max_length=150,
        num_beams=2,
        repetition_penalty=2.5,
        length_penalty=1.0,
        
        # Turn this to False for a much cleaner output every time.
        early_stopping=True
    )

    # Predictions returned are nothing but an array of words.
    predictions = [
        tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        for gen_id in generated_ids
    ]

    return "".join(predictions)

"""## Perform Summarization of a Random Sample"""

sampleData = test_df.iloc[0]

text = sampleData['text']
modelSummary = summarize(text)

"""## Compare Text to Model Summary

### Text Data
"""

text

"""### Summary from Data"""

sampleData['summary']

"""### Model Summary"""

modelSummary

"""# Evaluating Model"""